{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ember\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "binary_name = \"mimikatz.exe\" #set this to the binary name, put the ones u want to test in /binaries\n",
    "\n",
    "data_dir = \"../ember2018/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_location = f\"/workspaces/torment-nexus/binaries/{binary_name}\"\n",
    "\n",
    "def get_feature_names() -> list[str]:\n",
    "    byte_histogram = [f\"Byte Histogram {a}\" for a in range(256)] #256\n",
    "\n",
    "    byte_entropy_histogram = [f\"Byte Entropy Histogram {a}\" for a in range(256)] #256\n",
    "\n",
    "    strings_1 = [f\"string.{a}\" for a in [\"numstrings\", \"avlength\", \"printables\"]]\n",
    "    strings_2 = [f\"string.printabledist_{b}\" for b in range(96)]\n",
    "    strings_3 = [f\"string.{a}\" for a in [\"entropy\", \"paths\", \"urls\", \"registry\", \"MZ\"]] # 8 + 96\n",
    "    strings = np.concatenate((strings_1,strings_2,strings_3))\n",
    "\n",
    "    general_info = [f\"general.{a}\" for a in [\"size\", \"vsize\", \"has_debug\", \"exports\", \"imports\", \"has_relocations\", \"has_resources\", \"has_signature\", \"has_tls\", \"symbols\"]]\n",
    "\n",
    "    header_coff = [\"header.coff.timestamp\"]\n",
    "    header_coff_machine = [f\"header.coff.machine_{a}\" for a in range(10)]\n",
    "    header_coff_characteristics = [f\"header.coff.characteristic_{a}\" for a in range(10)]\n",
    "    header_coff_subsystem = [f\"header.optional.subsystem_{a}\" for a in range(10)]\n",
    "    header_coff_dll_characteristics = [f\"header.optional.dll_characteristic_{a}\" for a in range(10)]\n",
    "    header_coff_magic = [f\"header.optional.magic_{a}\" for a in range(10)]\n",
    "    header_optional = [f\"header.optional.{a}\" for a in [\"major_image_version\", \"minor_image_version\", \"major_linker_version\", \"minor_linker_version\", \"major_operating_system_version\", \"minor_operating_system_version\", \"major_subsystem_version\", \"minor_subsystem_version\", \"sizeof_code\", \"sizeof_headers\", \"sizeof_heap_commit\"]] #12\n",
    "    header = np.concatenate((header_coff,header_coff_machine,header_coff_characteristics,header_coff_subsystem,header_coff_dll_characteristics,header_coff_magic,header_optional))\n",
    "\n",
    "    sections_general = [f\"sections.{a}\" for a in [\"section_count\", \"num_empty_sections\", \"num_unnamed_sections\", \"num_read_execute_sections\", \"num_write_sections\",]] #JUST general\n",
    "    sections_section_sizes = [f\"sections.section_{a}_size\" for a in range(50)] # this messes with hashing which i will understand at a later time\n",
    "    sections_section_entropy = [f\"sections.section_{a}_entropy\" for a in range(50)]\n",
    "    sections_section_vsize = [f\"sections.section_{a}_vsize\" for a in range(50)]\n",
    "    sections_entry_name = [f\"sections.entry_name_{a}\" for a in range(50)]\n",
    "    sections_characteristics = [f\"sections.characteristics_{a}\" for a in range(50)]\n",
    "    sections = np.concatenate((sections_general, sections_section_sizes, sections_section_entropy, sections_section_vsize, sections_entry_name, sections_characteristics))\n",
    "\n",
    "    imports_libraries = [f\"imports.libraries.library_{a}\" for a in range(256)]\n",
    "    imports_imports = [f\"imports.import_{a}\" for a in range(1024)]\n",
    "    imports = np.concatenate((imports_libraries,imports_imports))\n",
    "\n",
    "    exports = [f\"exports.export_{a}\" for a in range(128)]\n",
    "\n",
    "    name_order = [a.lower() for a in [\"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"]]\n",
    "    data_directories_unflat = [[f\"directories.{a}_size\", f\"directories.{a}_vaddress\"] for a in name_order]\n",
    "    data_directories = [item for sublist in data_directories_unflat for item in sublist]\n",
    "\n",
    "\n",
    "    feature_names = np.concatenate((byte_histogram, byte_entropy_histogram, strings, general_info, header, sections, imports, exports, data_directories))\n",
    "    return feature_names\n",
    "\n",
    "def classify_binary(binary_location:str) -> float:\n",
    "    lgbm_model = lgb.Booster(model_file=os.path.join(data_dir, \"ember_model_2018.txt\"))\n",
    "    extractor2 = ember.PEFeatureExtractor(2)\n",
    "\n",
    "    file_data = open(binary_location, \"rb\").read()\n",
    "    feature_vector = extractor2.feature_vector(file_data)\n",
    "\n",
    "    return lgbm_model.predict([np.array(feature_vector, dtype=np.float32)])[0]\n",
    "\n",
    "def classify_vectors(feature_vector:np.ndarray | pd.DataFrame) -> float:\n",
    "    if type(feature_vector) is pd.DataFrame:\n",
    "        feature_vector = feature_vector.to_numpy()\n",
    "\n",
    "    lgbm_model = lgb.Booster(model_file=os.path.join(data_dir, \"ember_model_2018.txt\"))\n",
    "    return lgbm_model.predict([np.array(feature_vector, dtype=np.float32)])[0]\n",
    "\n",
    "def get_vectors(binary_location:str) -> np.ndarray:\n",
    "    extractor2 = ember.PEFeatureExtractor(2)\n",
    "    file_data = open(binary_location, \"rb\").read()\n",
    "    return extractor2.feature_vector(file_data)\n",
    "\n",
    "def get_dataframe(feature_vector:np.ndarray) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(feature_vector).T\n",
    "    df.columns = get_feature_names()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find the section associated with EXPORT_TABLE\n",
      "Can't read the export table at 0xffffffff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Byte Histogram 0</th>\n",
       "      <th>Byte Histogram 1</th>\n",
       "      <th>Byte Histogram 2</th>\n",
       "      <th>Byte Histogram 3</th>\n",
       "      <th>Byte Histogram 4</th>\n",
       "      <th>Byte Histogram 5</th>\n",
       "      <th>Byte Histogram 6</th>\n",
       "      <th>Byte Histogram 7</th>\n",
       "      <th>Byte Histogram 8</th>\n",
       "      <th>Byte Histogram 9</th>\n",
       "      <th>...</th>\n",
       "      <th>directories.load_config_table_size</th>\n",
       "      <th>directories.load_config_table_vaddress</th>\n",
       "      <th>directories.bound_import_size</th>\n",
       "      <th>directories.bound_import_vaddress</th>\n",
       "      <th>directories.iat_size</th>\n",
       "      <th>directories.iat_vaddress</th>\n",
       "      <th>directories.delay_import_descriptor_size</th>\n",
       "      <th>directories.delay_import_descriptor_vaddress</th>\n",
       "      <th>directories.clr_runtime_header_size</th>\n",
       "      <th>directories.clr_runtime_header_vaddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.206864</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.00417</td>\n",
       "      <td>0.00469</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.00268</td>\n",
       "      <td>0.00986</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2416.0</td>\n",
       "      <td>610304.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>912092.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 2381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Byte Histogram 0  Byte Histogram 1  Byte Histogram 2  Byte Histogram 3  \\\n",
       "0          0.206864          0.008583           0.00417           0.00469   \n",
       "\n",
       "   Byte Histogram 4  Byte Histogram 5  Byte Histogram 6  Byte Histogram 7  \\\n",
       "0          0.007864          0.002909          0.002785           0.00268   \n",
       "\n",
       "   Byte Histogram 8  Byte Histogram 9  ...  \\\n",
       "0           0.00986          0.001998  ...   \n",
       "\n",
       "   directories.load_config_table_size  directories.load_config_table_vaddress  \\\n",
       "0                                 0.0                                     0.0   \n",
       "\n",
       "   directories.bound_import_size  directories.bound_import_vaddress  \\\n",
       "0                            0.0                                0.0   \n",
       "\n",
       "   directories.iat_size  directories.iat_vaddress  \\\n",
       "0                2416.0                  610304.0   \n",
       "\n",
       "   directories.delay_import_descriptor_size  \\\n",
       "0                                      96.0   \n",
       "\n",
       "   directories.delay_import_descriptor_vaddress  \\\n",
       "0                                      912092.0   \n",
       "\n",
       "   directories.clr_runtime_header_size  \\\n",
       "0                                  0.0   \n",
       "\n",
       "   directories.clr_runtime_header_vaddress  \n",
       "0                                      0.0  \n",
       "\n",
       "[1 rows x 2381 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataframe(get_vectors(binary_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find the section associated with EXPORT_TABLE\n",
      "Can't read the export table at 0xffffffff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score so far: [2.06864238e-01 8.58298130e-03 4.16987715e-03 ... 9.12092000e+05\n",
      " 0.00000000e+00 0.00000000e+00], iteration #1\n",
      "Score so far: [2.06864238e-01 8.58298130e-03 4.16987715e-03 ... 9.12092000e+05\n",
      " 0.00000000e+00 0.00000000e+00], iteration #2\n",
      "Score so far: [2.06864238e-01 8.58298130e-03 4.16987715e-03 ... 9.12092000e+05\n",
      " 0.00000000e+00 0.00000000e+00], iteration #3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     particles[i, j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m velocities[i, j]\n\u001b[1;32m     42\u001b[0m     particles[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(particles[i, j], bounds[j][\u001b[38;5;241m0\u001b[39m], bounds[j][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 43\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m personal_best_scores[i]:\n\u001b[1;32m     45\u001b[0m     personal_best_scores[i] \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(df):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassify_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 59\u001b[0m, in \u001b[0;36mclassify_vectors\u001b[0;34m(feature_vector)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(feature_vector) \u001b[38;5;129;01mis\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     57\u001b[0m     feature_vector \u001b[38;5;241m=\u001b[39m feature_vector\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m---> 59\u001b[0m lgbm_model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43member_model_2018.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lgbm_model\u001b[38;5;241m.\u001b[39mpredict([np\u001b[38;5;241m.\u001b[39marray(feature_vector, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightgbm/basic.py:3465\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3463\u001b[0m     \u001b[38;5;66;03m# Prediction task\u001b[39;00m\n\u001b[1;32m   3464\u001b[0m     out_num_iterations \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3465\u001b[0m     _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterCreateFromModelfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_iterations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3469\u001b[0m     out_num_class \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3470\u001b[0m     _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterGetNumClasses(\n\u001b[1;32m   3471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[1;32m   3472\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(out_num_class)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def objective_function(df):\n",
    "    return classify_vectors(df)\n",
    "\n",
    "df = get_dataframe(get_vectors(binary_location))\n",
    "\n",
    "num_particles = 30\n",
    "num_iterations = 100\n",
    "dim = df.shape[1]\n",
    "c1 = c2 = 1.49445\n",
    "w = 0.729\n",
    "boundsdict = {\"header.coff.timestamp\": (0, 0xFFFFFFFF), \"directories.certificate_table_size\": (0,0xFFFFFFFF)}\n",
    "changeable_str = [\"header.coff.timestamp\", \"directories.certificate_table_size\"]\n",
    "bounds = []\n",
    "changeable = []\n",
    "for index,feature in enumerate(df):\n",
    "    if feature not in changeable_str:\n",
    "        bounds.append((df[feature].iloc[0], df[feature].iloc[0]))\n",
    "    else:\n",
    "        bounds.append(boundsdict[feature])\n",
    "        changeable.append(index)\n",
    "\n",
    "particles = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds], (num_particles, dim))\n",
    "velocities = np.random.uniform(-1, 1, (num_particles, dim))\n",
    "personal_best_positions = np.copy(particles)\n",
    "personal_best_scores = np.array([objective_function(p) for p in particles])\n",
    "global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    for i in range(num_particles):\n",
    "        for j in changeable:\n",
    "            velocities[i, j] = (w * velocities[i, j] +\n",
    "                                c1 * np.random.rand() * (personal_best_positions[i, j] - particles[i, j]) +\n",
    "                                c2 * np.random.rand() * (global_best_position[j] - particles[i, j]))\n",
    "            particles[i, j] += velocities[i, j]\n",
    "            particles[i, j] = np.clip(particles[i, j], bounds[j][0], bounds[j][1])\n",
    "        score = objective_function(particles[i])\n",
    "        if score < personal_best_scores[i]:\n",
    "            personal_best_scores[i] = score\n",
    "            personal_best_positions[i] = particles[i]\n",
    "            \n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    print(f\"Score so far: {global_best_position}, iteration #{iteration+1}\")\n",
    "\n",
    "print(\"Best position:\", global_best_position)\n",
    "print(\"Best score:\", objective_function(global_best_position))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,feature in enumerate(df):\n",
    "    print(f\"{feature}, {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
